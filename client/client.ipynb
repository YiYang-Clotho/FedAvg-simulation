{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d03d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = 1\n",
    "\n",
    "C = 1\n",
    "E = 1\n",
    "B = 10 # 'all' for a single minibatch\n",
    "\n",
    "rounds = 10 # default\n",
    "local_epochs = 1 # default\n",
    "lr = 0.1\n",
    "\n",
    "# loss='categorical_crossentropy'\n",
    "# metrics = ['accuracy']\n",
    "# cce = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a725e860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 05:49:10.200806: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-25 05:49:11.098143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import timedelta\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tqdm import tqdm\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "import h5py\n",
    "import socket\n",
    "import struct\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c65c908d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_order(start from 0): 0\n"
     ]
    }
   ],
   "source": [
    "client_order = int(input(\"client_order(start from 0): \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da85be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_traindata = 6000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c36b7b",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "818df36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39a19991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noniid_partition(y_train):\n",
    "    n_shards = 20\n",
    "    n_per_shard = 3000\n",
    "    \n",
    "    indexes_per_client = {}\n",
    "    indexes = y_train.argsort()\n",
    "    \n",
    "    indexes_shard = np.arange(0, n_shards)\n",
    "#     random.shuffle(indexes_shard)\n",
    "    \n",
    "    start_idx_shard_1 = indexes_shard[client_order*2]*n_per_shard\n",
    "    start_idx_shard_2 = indexes_shard[n_shards - (client_order+1)]*n_per_shard\n",
    "    indexes_per_client[client_order] = np.concatenate((indexes[start_idx_shard_1:start_idx_shard_1+n_per_shard],\n",
    "                                                       indexes[start_idx_shard_2:start_idx_shard_2+n_per_shard]))\n",
    "    print(client_order, \":\", start_idx_shard_1, start_idx_shard_1+n_per_shard-1, start_idx_shard_2, start_idx_shard_2+n_per_shard-1)\n",
    "    \n",
    "    return indexes_per_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b87bd324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0 2999 57000 59999\n"
     ]
    }
   ],
   "source": [
    "indexes_per_client = noniid_partition(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2c4ebcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([30207,  5662, 55366, ..., 23285, 15728, 11924])}\n"
     ]
    }
   ],
   "source": [
    "print(indexes_per_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe4b4d1",
   "metadata": {},
   "source": [
    "Normalize, Expand Dims, and Transform Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1247e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype(\"float32\")/255\n",
    "X_test = X_test.astype(\"float32\")/255\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(\"x_train shape:\", X_train.shape)\n",
    "print(X_train.shape[0], \"train samples\")\n",
    "print(X_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15925a6",
   "metadata": {},
   "source": [
    "### Create Batched Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d15826c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(indexes_client, X_train, y_train, B):\n",
    "    x = []\n",
    "    y = []    \n",
    "    for i in indexes_client:\n",
    "      x.append(X_train[i])\n",
    "      y.append(y_train[i])\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(x), list(y)))\n",
    "    return dataset.shuffle(len(y)).batch(len(y_train) if B=='all' else B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "080012df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                        | 0/1 [00:00<?, ?it/s]2023-04-25 05:49:17.622399: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.04s/it]\n"
     ]
    }
   ],
   "source": [
    "client_dataset_batched = {}\n",
    "for i, indexes in tqdm(indexes_per_client.items()):\n",
    "  client_dataset_batched[i] = create_batch(indexes, X_train, y_train, B)\n",
    "\n",
    "train_batched = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(len(y_train)) # for testing on train set\n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b6c83a",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91e7ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    @staticmethod\n",
    "    def build(input_shape):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=input_shape))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(filters=64, padding='same', kernel_size=(5,5), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ccda608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        832       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)        51264     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1606144   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,663,370\n",
      "Trainable params: 1,663,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "local_model = model.build((28,28,1))\n",
    "local_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "043c9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 100 # default\n",
    "local_epochs = 1 # default\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff47a0e",
   "metadata": {},
   "source": [
    "## Socket initialization\n",
    "### Required socket functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5fad5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_msg(sock, msg):\n",
    "    # prefix each message with a 4-byte length in network byte order\n",
    "    msg = pickle.dumps(msg)\n",
    "    msg = struct.pack('>I', len(msg)) + msg\n",
    "    sock.sendall(msg)\n",
    "\n",
    "def recv_msg(sock):\n",
    "    # read message length and unpack it into an integer\n",
    "    raw_msglen = recvall(sock, 4)\n",
    "    if not raw_msglen:\n",
    "        return None\n",
    "    msglen = struct.unpack('>I', raw_msglen)[0]\n",
    "    # read the message data\n",
    "    msg =  recvall(sock, msglen)\n",
    "    msg = pickle.loads(msg)\n",
    "    return msg\n",
    "\n",
    "def recvall(sock, n):\n",
    "    # helper function to receive n bytes or return None if EOF is hit\n",
    "    data = b''\n",
    "    while len(data) < n:\n",
    "        packet = sock.recv(n - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd2a052",
   "metadata": {},
   "source": [
    "### Set host address and port number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97678404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP address: 172.31.2.147\n"
     ]
    }
   ],
   "source": [
    "host_name = input(\"IP address: \")\n",
    "port_number = 12345\n",
    "max_recv = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef6fb42",
   "metadata": {},
   "source": [
    "### Open the client socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "306b0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = socket.socket()\n",
    "s.connect((host_name, port_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add446f1",
   "metadata": {},
   "source": [
    "## SET TIMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd4b6f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timmer start!\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()    # store start time\n",
    "print(\"timmer start!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26813ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = recv_msg(s)\n",
    "rounds = msg['rounds'] \n",
    "client_id = msg['client_id']\n",
    "local_epochs = msg['local_epoch']\n",
    "send_msg(s, len(train_batched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ac863e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a451b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_weights = recv_msg(s)\n",
    "# local_model.set_weights(global_weights)\n",
    "\n",
    "# optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=lr)\n",
    "# local_model.compile(loss=loss,\n",
    "#                     optimizer=optimizer,\n",
    "#                     metrics=metrics)\n",
    "# history = local_model.fit(client_dataset_batched[client_order], epochs=E, verbose=1)\n",
    "# evaluate = local_model.evaluate(test_batched)\n",
    "\n",
    "# msg = evaluate\n",
    "# send_msg(s, msg)\n",
    "\n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96ce9adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 05:49:31.921922: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [6000,10]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-04-25 05:49:31.922439: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [6000,10]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 14s 14ms/step - loss: 0.0610 - accuracy: 0.9807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 05:49:45.900226: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [10000,10]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 7.4915 - accuracy: 0.1973\n",
      "600/600 [==============================] - 8s 13ms/step - loss: 0.0115 - accuracy: 0.9963\n",
      "1/1 [==============================] - 2s 2s/step - loss: 11.5163 - accuracy: 0.1975\n",
      "600/600 [==============================] - 8s 13ms/step - loss: 0.0074 - accuracy: 0.9970\n",
      "1/1 [==============================] - 2s 2s/step - loss: 7.5780 - accuracy: 0.1982\n",
      "600/600 [==============================] - 8s 13ms/step - loss: 0.0051 - accuracy: 0.9982\n",
      "1/1 [==============================] - 2s 2s/step - loss: 8.5677 - accuracy: 0.1984\n",
      "600/600 [==============================] - 8s 13ms/step - loss: 0.0017 - accuracy: 0.9995\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.8251 - accuracy: 0.1985\n",
      "600/600 [==============================] - 8s 13ms/step - loss: 9.1436e-04 - accuracy: 0.9997\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.7990 - accuracy: 0.1984\n",
      "600/600 [==============================] - 8s 13ms/step - loss: 2.4571e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 11.5614 - accuracy: 0.1986\n",
      "600/600 [==============================] - 8s 13ms/step - loss: 1.5178e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 12.3510 - accuracy: 0.1985\n",
      "600/600 [==============================] - 8s 13ms/step - loss: 1.0560e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 12.7083 - accuracy: 0.1986\n",
      "600/600 [==============================] - 8s 13ms/step - loss: 7.9771e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 13.0548 - accuracy: 0.1986\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# update weights from server\n",
    "# train\n",
    "for r in range(rounds):  # loop over the dataset multiple times\n",
    "    global_weights = recv_msg(s)\n",
    "#     local_model.load_weights(global_weights)\n",
    "    local_model.set_weights(global_weights)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=lr)\n",
    "    local_model.compile(loss=loss,\n",
    "                        optimizer=optimizer,\n",
    "                        metrics=metrics)\n",
    "    history = local_model.fit(client_dataset_batched[client_order], epochs=E, verbose=1)\n",
    "    evaluate = local_model.evaluate(test_batched)\n",
    "\n",
    "    msg = local_model.get_weights()\n",
    "    send_msg(s, msg)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d209ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 111.82278966903687 sec\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()  #store end time\n",
    "print(\"Training Time: {} sec\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee3ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
