{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37ed6969",
   "metadata": {},
   "source": [
    "# Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de770af",
   "metadata": {},
   "source": [
    "## Setting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99f6019",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 10\n",
    "local_epoch = 1\n",
    "users = 2 # number of clients\n",
    "target_test_acc = 0.99\n",
    "lr = 0.1\n",
    "\n",
    "C = 1\n",
    "E = 5\n",
    "B = 10 # 'all' for a single minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ecf253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import socket\n",
    "import struct\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from threading import Thread\n",
    "from threading import Lock\n",
    "\n",
    "import copy\n",
    "\n",
    "import logging\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import timedelta\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tqdm import tqdm\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e026b7",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca54d616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = (\"gpu\" if tf.test.gpu_device_name() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20256a47",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab65c69a",
   "metadata": {},
   "source": [
    "A CNN with two 5x5 convolution layers (the first with 32 channels, the second with 64, each followed with 2x2 max pooling), a fully connected layer with 512 units and ReLu activation, and a final softmax output layer (1,663,370 total parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95101fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    @staticmethod\n",
    "    def build(input_shape):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=input_shape))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(filters=64, padding='same', kernel_size=(5,5), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed60a1e",
   "metadata": {},
   "source": [
    "initialize global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f4a3388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        832       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)        51264     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1606144   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,663,370\n",
      "Trainable params: 1,663,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "  \n",
    "global_model = model.build((28,28,1))\n",
    "initial_weights = global_model.get_weights()\n",
    "\n",
    "# client_models = [model.build((28,28,1)) for _ in range(K)]\n",
    "\n",
    "# for i in range(len(client_models)):\n",
    "#   client_models[i].compile(loss=loss, \n",
    "#                       optimizer=optimizer, \n",
    "#                       metrics=metrics)\n",
    "#   client_models[i].set_weights(global_model.get_weights())\n",
    "\n",
    "global_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1386ff9e",
   "metadata": {},
   "source": [
    "## variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9b946d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientsoclist = [0]*users\n",
    "\n",
    "start_time = 0\n",
    "weight_count = 0\n",
    "\n",
    "global_weights = initial_weights\n",
    "\n",
    "datasetsize = [0]*users\n",
    "weights_list = [0]*users\n",
    "\n",
    "lock = Lock()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0136545a",
   "metadata": {},
   "source": [
    "## Comunication overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6cc04bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sendsize_list = []\n",
    "total_receivesize_list = []\n",
    "\n",
    "client_sendsize_list = [[] for i in range(users)]\n",
    "client_receivesize_list = [[] for i in range(users)]\n",
    "\n",
    "train_sendsize_list = [] \n",
    "train_receivesize_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d2aa8",
   "metadata": {},
   "source": [
    "## Socket initialization\n",
    "### Set host address and port number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf47e0c",
   "metadata": {},
   "source": [
    "### Required socket functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bdef5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_msg(sock, msg):\n",
    "    # prefix each message with a 4-byte length in network byte order\n",
    "    msg = pickle.dumps(msg)\n",
    "    l_send = len(msg)\n",
    "    msg = struct.pack('>I', l_send) + msg\n",
    "    sock.sendall(msg)\n",
    "    return l_send\n",
    "\n",
    "def recv_msg(sock):\n",
    "    # read message length and unpack it into an integer\n",
    "    raw_msglen = recvall(sock, 4)\n",
    "    if not raw_msglen:\n",
    "        return None\n",
    "    msglen = struct.unpack('>I', raw_msglen)[0]\n",
    "    # read the message data\n",
    "    msg =  recvall(sock, msglen)\n",
    "    msg = pickle.loads(msg)\n",
    "    return msg, msglen\n",
    "\n",
    "def recvall(sock, n):\n",
    "    # helper function to receive n bytes or return None if EOF is hit\n",
    "    data = b''\n",
    "    while len(data) < n:\n",
    "        packet = sock.recv(n - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85d51008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def average_weights(w, datasize):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "    avg_weights = list()\n",
    "    for j in range(len(global_weights)):\n",
    "        weights = [w[k][j] for k in range(len(w))]\n",
    "        layer_mean = tf.math.reduce_mean(weights, axis=0)\n",
    "        avg_weights.append(layer_mean)\n",
    "        \n",
    "    return avg_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08589b19",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57f4a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(userid, train_dataset_size, num_users, client_conn):\n",
    "    global weights_list\n",
    "    global global_weights\n",
    "    global weight_count\n",
    "    global val_acc\n",
    "    \n",
    "    for r in range(rounds):\n",
    "        with lock:\n",
    "            if weight_count == num_users:\n",
    "                for i, conn in enumerate(clientsoclist):\n",
    "                    datasize = send_msg(conn, global_weights)\n",
    "                    total_sendsize_list.append(datasize)\n",
    "                    client_sendsize_list[i].append(datasize)\n",
    "                    train_sendsize_list.append(datasize)\n",
    "                    weight_count = 0\n",
    "\n",
    "        client_weights, datasize = recv_msg(client_conn)\n",
    "        total_receivesize_list.append(datasize)\n",
    "        client_receivesize_list[userid].append(datasize)\n",
    "        train_receivesize_list.append(datasize)\n",
    "\n",
    "        weights_list[userid] = client_weights\n",
    "        print(\"User\" + str(userid) + \"'s Round \" + str(r + 1) +  \" is done\")\n",
    "        with lock:\n",
    "            weight_count += 1\n",
    "            if weight_count == num_users:\n",
    "                #average\n",
    "                global_weights = average_weights(weights_list, datasetsize)\n",
    "#                 global_model.set_weights(global_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78fdb23",
   "metadata": {},
   "source": [
    "## Receive users before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76b5c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def receive(userid, num_users, conn): #thread for receive clients\n",
    "    global weight_count\n",
    "    global datasetsize\n",
    "\n",
    "\n",
    "    msg = {\n",
    "        'rounds': rounds,\n",
    "        'client_id': userid,\n",
    "        'local_epoch': local_epoch\n",
    "    }\n",
    "\n",
    "    datasize = send_msg(conn, msg)    #send epoch\n",
    "    total_sendsize_list.append(datasize)\n",
    "    client_sendsize_list[userid].append(datasize)\n",
    "\n",
    "    train_dataset_size, datasize = recv_msg(conn)    # get total_batch of train dataset\n",
    "    total_receivesize_list.append(datasize)\n",
    "    client_receivesize_list[userid].append(datasize)\n",
    "    \n",
    "    \n",
    "    with lock:\n",
    "        datasetsize[userid] = train_dataset_size\n",
    "        weight_count += 1\n",
    "    \n",
    "    train(userid, train_dataset_size, num_users, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b60e7",
   "metadata": {},
   "source": [
    "## Thread define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c48a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_thread(func, num_user):\n",
    "    global clientsoclist\n",
    "    global start_time\n",
    "    \n",
    "    thrs = []\n",
    "    for i in range(num_user):\n",
    "        conn, addr = s.accept()\n",
    "        print('Conntected with', addr)\n",
    "        # append client socket on list\n",
    "        clientsoclist[i] = conn\n",
    "        args = (i, num_user, conn)\n",
    "        thread = Thread(target=func, args=args)\n",
    "        thrs.append(thread)\n",
    "        thread.start()\n",
    "    print(\"timmer start!\")\n",
    "    start_time = time.time()    # store start time\n",
    "    for thread in thrs:\n",
    "        thread.join()\n",
    "    end_time = time.time()  # store end time\n",
    "    print(\"TrainingTime: {} sec\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16a79683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.16.2.211\n"
     ]
    }
   ],
   "source": [
    "host_name = socket.gethostbyname(socket.gethostname())\n",
    "port_number = 12345\n",
    "print(host_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "549e34a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = socket.socket()\n",
    "s.bind((host_name, port_number))\n",
    "s.listen(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c433d",
   "metadata": {},
   "source": [
    "### Open the server socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "710c7465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conntected with ('172.16.2.211', 63390)\n",
      "Conntected with ('172.16.2.211', 63391)\n",
      "timmer start!\n",
      "User0's Round 1 is doneUser1's Round 1 is done\n",
      "\n",
      "User0's Round 2 is done\n",
      "User1's Round 2 is done\n",
      "User0's Round 3 is done\n",
      "User1's Round 3 is done\n",
      "User0's Round 4 is done\n",
      "User1's Round 4 is done\n",
      "User0's Round 5 is doneUser1's Round 5 is done\n",
      "\n",
      "User0's Round 6 is done\n",
      "User1's Round 6 is done\n",
      "User0's Round 7 is done\n",
      "User1's Round 7 is done\n",
      "User0's Round 8 is done\n",
      "User1's Round 8 is done\n",
      "User0's Round 9 is doneUser1's Round 9 is done\n",
      "\n",
      "User1's Round 10 is done\n",
      "User0's Round 10 is done\n",
      "TrainingTime: 161.2054409980774 sec\n"
     ]
    }
   ],
   "source": [
    "run_thread(receive, users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7507fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingTime: 161.21686220169067 sec\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()  # store end time\n",
    "print(\"TrainingTime: {} sec\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe889bf",
   "metadata": {},
   "source": [
    "## Print all of communication overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4090b261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "total_sendsize size: 133079930 bytes\n",
      "\n",
      "\n",
      "total receive sizes: 133077990 bytes\n",
      "\n",
      "\n",
      "client_sendsize_list(user0): \n",
      "total client_sendsizes(user0): 66539965 bytes\n",
      "\n",
      "\n",
      "total client_receive sizes(user0): 66538995 bytes\n",
      "\n",
      "\n",
      "client_sendsize_list(user1): \n",
      "total client_sendsizes(user1): 66539965 bytes\n",
      "\n",
      "\n",
      "total client_receive sizes(user1): 66538995 bytes\n",
      "\n",
      "\n",
      "total train_sendsizes: 133079816 bytes\n",
      "\n",
      "\n",
      "total train_receivesizes: 133077980 bytes\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print('val_acc list')\n",
    "# for acc in val_acc:\n",
    "#     print(acc)\n",
    "\n",
    "print('\\n')\n",
    "total_size = 0\n",
    "for size in total_sendsize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total_sendsize size: {} bytes\".format(total_size))\n",
    "print('\\n')\n",
    "\n",
    "total_size = 0\n",
    "for size in total_receivesize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total receive sizes: {} bytes\".format(total_size) )\n",
    "print('\\n')\n",
    "\n",
    "for i in range(users):\n",
    "    print('client_sendsize_list(user{}): '.format(i))\n",
    "    total_size = 0\n",
    "    for size in client_sendsize_list[i]:\n",
    "#         print(size)\n",
    "        total_size += size\n",
    "    print(\"total client_sendsizes(user{}): {} bytes\".format(i, total_size))\n",
    "    print('\\n')\n",
    "\n",
    "    total_size = 0\n",
    "    for size in client_receivesize_list[i]:\n",
    "#         print(size)\n",
    "        total_size += size\n",
    "    print(\"total client_receive sizes(user{}): {} bytes\".format(i, total_size))\n",
    "    print('\\n')\n",
    "\n",
    "total_size = 0\n",
    "for size in train_sendsize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total train_sendsizes: {} bytes\".format(total_size))\n",
    "print('\\n')\n",
    "\n",
    "total_size = 0\n",
    "for size in train_receivesize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total train_receivesizes: {} bytes\".format(total_size))\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2741eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3271172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype(\"float32\")/255\n",
    "X_test = X_test.astype(\"float32\")/255\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(\"x_train shape:\", X_train.shape)\n",
    "print(X_train.shape[0], \"train samples\")\n",
    "print(X_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08866f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batched = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(len(y_train)) # for testing on train set\n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e573a14f",
   "metadata": {},
   "source": [
    "## Accuracy of train and each of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f60ac521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate: 0.1\n",
      "1875/1875 [==============================] - 10s 5ms/step\n",
      "313/313 [==============================] - 2s 5ms/step\n",
      "comm_round: 10 | test_acc: 46.180% | test_loss: 5.320533752441406 | train_acc: 46.368% | train_loss: 5.233935356140137 | elapsed: 0:00:12.976362\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'builtin_function_or_method' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-49f7d89c1108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m'test_losses'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                       }\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'result_per_lr_{}_{}_{}_{}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_per_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'builtin_function_or_method' and 'str'"
     ]
    }
   ],
   "source": [
    "result_per_lr = {}\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "\n",
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "start = time.time()\n",
    "# optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=lr)\n",
    "# global_model.set_weights(initial_weights)\n",
    "print('\\nlearning rate: {}'.format(lr))\n",
    "# for r in range(rounds):\n",
    "# global_weights = global_model.get_weights()\n",
    "global_model.set_weights(global_weights)\n",
    "\n",
    "# test global model on full training set\n",
    "for (X,y) in train_batched:\n",
    "  preds = global_model.predict(X)\n",
    "  train_loss = cce(y, preds)\n",
    "  train_acc = accuracy_score(tf.argmax(preds, axis=1), tf.argmax(y, axis=1))\n",
    "  train_losses.append(train_loss.numpy())\n",
    "  train_accs.append(train_acc)\n",
    "\n",
    "# test global model on testing set\n",
    "for(X, y) in test_batched:\n",
    "  preds = global_model.predict(X)\n",
    "  test_loss = cce(y, preds)\n",
    "  test_acc = accuracy_score(tf.argmax(preds, axis=1), tf.argmax(y, axis=1))\n",
    "  test_losses.append(test_loss.numpy())\n",
    "  test_accs.append(test_acc)\n",
    "\n",
    "elapsed = (time.time() - start)\n",
    "\n",
    "print('comm_round: {} | test_acc: {:.3%} | test_loss: {} | train_acc: {:.3%} | train_loss: {} | elapsed: {}'.format(rounds, test_acc, test_loss, train_acc, train_loss, timedelta(seconds=elapsed)))\n",
    "\n",
    "result_per_lr[lr] = {\n",
    "    'train_accs' : train_accs,\n",
    "    'test_accs' : test_accs,\n",
    "    'train_losses' : train_losses,\n",
    "    'test_losses' : test_losses\n",
    "                      }\n",
    "with open(dir+'result_per_lr_{}_{}_{}_{}.pickle'.format(B,C,E, lr), 'wb') as handle:\n",
    "    pickle.dump(result_per_lr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07fcab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()  # store end time\n",
    "print(\"WorkingTime: {} sec\".format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
